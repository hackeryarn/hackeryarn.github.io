<!DOCTYPE html>
<html class="latte" lang="en">

<head>
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge" />
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, viewport-fit=cover" />
  <meta name="fediverse:creator" content="@hackeryarn@mastodon.social">
  
  
  
  <title>hackeryarn | Efficient CSV Imports in Rails</title>
  
  <link rel="icon" type="image/svg" href="/imgs/hackeryarn.svg" />
  <link rel="stylesheet" href="/css/generated.css">
  
  <link rel="alternate" type="application/atom+xml" title="Atom Feed for https://hackeryarn.com"
    href="/atom.xml" />
  
  <script defer src="/js/main.bundle.js" data-auto-replace-svg="nest"></script>
</head>

<body hx-boost="true" hx-ext="head-support" class="flex flex-col justify-between h-screen">
  <header class="top-0 w-full z-10 flex justify-between items-center mb-7 lg:text-8xl text-4xl p-5">
  <a class="logo flex items-center no-underline lg:gap-5 gap-2 neutral-900 italianno" href="https://hackeryarn.com">
    <img class="inline lg:w-20 w-10 rounded-full" src="/imgs/hackeryarn.svg" /> hackeryarn
  </a>
  <nav class="carrois lg:text-3xl text-lg">
    <ul class="flex gap-3 lg:gap-6">
      <li>
        <a href="https://hackeryarn.com">posts</a>
      </li>
      <li>
        <a href="https://hackeryarn.com/tags">tags</a>
      </li>
      <li>
        <a href="https://hackeryarn.com/about">about</a>
      </li>
    </ul>
  </nav>
</header>
  <section class="w-full mx-auto mb-auto prose-neutral prose lg:prose-2xl px-6">
    
<article>
  <h1 class="title">
    Efficient CSV Imports in Rails
  </h1>
  <p class="subtitle">Published 2017-06-11</p>
  <div>
    <p>Rails has great capabilities for working CSV files. However, like with many
things, the most obvious way is not the most efficient.</p>
<span id="continue-reading"></span>
<p>We noticed this when our server had major fluctuations in memory consumption.
After digging through metrics, made easy thanks to
<a rel="external" href="https://prometheus.io">Prometheus</a> and <a rel="external" href="https://grafana.com">Grafana</a>. We noticed that the spikes were due to our CSV
uploads.</p>
<h1 id="examining-csv-import">Examining CSV Import</h1>
<p>Our processor is responsible for bringing in coordinates
from legacy systems and ones that cannot support our API.</p>
<p>The original code:</p>
<pre class="giallo z-code"><code data-lang="plain"><span class="giallo-l"><span>require &#39;csv&#39;</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>class CsvProcessor</span></span>
<span class="giallo-l"><span>  def self.import_locations(file)</span></span>
<span class="giallo-l"><span>    CSV.parse(file.read) do |row|</span></span>
<span class="giallo-l"><span>      Location.create(</span></span>
<span class="giallo-l"><span>        name: row[&#39;Name&#39;],</span></span>
<span class="giallo-l"><span>        lat: row[&#39;Lat&#39;],</span></span>
<span class="giallo-l"><span>        lon: row[&#39;Lon&#39;]</span></span>
<span class="giallo-l"><span>      )</span></span>
<span class="giallo-l"><span>    end</span></span>
<span class="giallo-l"><span>  end</span></span>
<span class="giallo-l"><span>end</span></span></code></pre>
<p>Looking at this code, I quickly came to the assumption that
<code>read</code> was storing the entire file in memory before parsing.
Which sent me on a search for a more efficient parsing method.</p>
<h1 id="better-parsing-method">Better Parsing Method</h1>
<p>Through my search, I came across an amazingly detailed <a rel="external" href="https://dalibornasevic.com/posts/68-processing-large-csv-files-with-ruby">article</a> on how to get the
most memory efficient reads possible. It turns out, we were using the least
efficient method to parse CSV files. I won't put all the statistics here because
the original article does a great deep dive on all the possibilities.
I will only include the most efficient method, and the one we used.</p>
<p>Efficient read code:</p>
<pre class="giallo z-code"><code data-lang="plain"><span class="giallo-l"><span>require &#39;csv&#39;</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>class CsvProcessor</span></span>
<span class="giallo-l"><span>  def self.import_locations(file)</span></span>
<span class="giallo-l"><span>    CSV.foreach(file) do |row|</span></span>
<span class="giallo-l"><span>      Location.create(</span></span>
<span class="giallo-l"><span>        name: row[&#39;Name&#39;],</span></span>
<span class="giallo-l"><span>        lat: row[&#39;Lat&#39;],</span></span>
<span class="giallo-l"><span>        lon: row[&#39;Lon&#39;]</span></span>
<span class="giallo-l"><span>      )</span></span>
<span class="giallo-l"><span>    end</span></span>
<span class="giallo-l"><span>  end</span></span>
<span class="giallo-l"><span>end</span></span></code></pre>
<p>The change was minor. I had to use <code>CSV.foreach</code> instead of <code>CSV.parse</code> which
performs a line by line streaming traversal of the file. When working with files
it is beneficial to have a stream. Streams only stores as much information as
needed during each cycle. In this case, it only needs one line.</p>
<p>I also got to eliminate the manual read. This cleaned up my code a bit, and I was
more than happy to let <code>CSV.foreach</code> handle the reading for me.</p>
<p>This one change eliminated our memory spikes. Making CSV imports a
minor event for the server.</p>
<h1 id="reducing-the-number-of-creates">Reducing the Number of Creates</h1>
<p>However, while looking a the metrics around CSV imports I also noticed that it
took a long time to import a CSV. The most glaring suspect was the <code>create</code> on
every row.</p>
<p>Rails libraries come to the rescue! There is a great library,
<a rel="external" href="https://github.com/zdennis/activerecord-import">activerecord-import</a>, which allows for a singe database transaction for multiple
creates and doesn't complicate the code much.</p>
<p>I eagerly tried to insert all the records in a single transaction. Although this
had some speed improvement, it shot up our memory consumption again. So I started
experimenting with intervals. With some trial and error, I arrived at 200
records per transaction. It didn't consume too much memory and was actually the
fastest. Anything below 200 created too many transactions and the efficiency
dropped. Anything above 200 was creating too large of transactions for them to
be efficiently saved.</p>
<p>My final code:</p>
<pre class="giallo z-code"><code data-lang="plain"><span class="giallo-l"><span>require &#39;csv&#39;</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>class CsvProcessor</span></span>
<span class="giallo-l"><span>  def self.import_locations(file)</span></span>
<span class="giallo-l"><span>    locations = []</span></span>
<span class="giallo-l"><span>    CSV.foreach(file) do |row|</span></span>
<span class="giallo-l"><span>      locations &lt;&lt; Location.new(</span></span>
<span class="giallo-l"><span>        name: row[&#39;Name&#39;],</span></span>
<span class="giallo-l"><span>        lat: row[&#39;Lat&#39;],</span></span>
<span class="giallo-l"><span>        lon: row[&#39;Lon&#39;]</span></span>
<span class="giallo-l"><span>      )</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>      if locations.length &gt; 200</span></span>
<span class="giallo-l"><span>        Location.import locations</span></span>
<span class="giallo-l"><span>        locations = []</span></span>
<span class="giallo-l"><span>      end</span></span>
<span class="giallo-l"><span>    end</span></span>
<span class="giallo-l"></span>
<span class="giallo-l"><span>    Location.import locations</span></span>
<span class="giallo-l"><span>  end</span></span>
<span class="giallo-l"><span>end</span></span></code></pre>
<p>This change required a bit more code. I had to maintain a list of locations to
import. Which I checked for the appropriate length, 200, every
iteration.</p>
<p>Once the file read was complete. I performed a final import to save
any remaining locations. If the list was empty, it would
result in no import.</p>
<p>This final change improved our import time over 10x and made this small project
much more worth it!</p>

  </div>
  <hr />
  <div class="not-prose -mt-10">
    

<ul class="not-prose flex gap-3">
  
  <li>
    <a href="https://hackeryarn.com/tags/tutorial/"
      class="prose lg:prose-2xl underline decoration-dotted transition text-neutral-500 hover:text-neutral-900">tutorial</a>
  </li>
  
</ul>


  </div>
</article>


  </section>
  <footer class="p-4 mt-15">
  <div class="flex justify-center text-4xl gap-6">
    <a href="https://mastodon.social/@hackeryarn">
      <i class="fa-brands fa-mastodon"></i>
    </a>
    <a href="https://codeberg.org/hackeryarn">
      <i class="codeberg"></i>
    </a>
    <a href="https://github.com/hackeryarn">
      <i class="fa-brands fa-github"></i>
    </a>
    <a href="https://www.linkedin.com/in/achernyak/">
      <i class="fa-brands fa-linkedin"></i>
    </a>
  </div>
  <p class="text-center text-neutral-500 mt-5">
    Copyright 2026 by Artem Chernyak
  </p>
</footer>
  <a class="hidden" rel="me" href="https://mastodon.social/@hackeryarn">Mastodon</a>
</body>

</html>